# -*- coding: utf-8 -*-
"""Waste Management Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/141E-OINc2BZB-H83B2o54EvwEzig7WZt

# Dataset

The Garbage Classification Dataset contains 6 classifications: cardboard (393), glass (491), metal (400), paper(584), plastic (472) and trash(127).

This dataset was obtained from a kaggle competition and some classes removed for this project 

The classes availlable for trainning and validation  in this project are  four classes

The dataset can be obtained from this link https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification


There are  four classes of waste material we are trying to classify 
1. Metal
2. Plastic
3. Paper
4. Glass

# Baseline Modeling

### Overview of Fast.ai
Fast.ai is a popular Deep Learning framework built on top of PyTorch. It aims at building state-of-the-art models quickly and easily in a few lines of code.  It greatly simplifies the training process of a deep learning model without compromising on the speed, flexibility, and performance of the training model. Fast.ai supports state-of-the-art techniques and models in Computer Vision and NLP too.

### Why should decided to  use Fast.ai?
Along with the high productivity and ease of using Fast.ai’s models, it also assures us the flexibility that enables us to customize the high-level API without meddling in the lower level. Fast.ai is also packed with some really cool features making it one of the beginner’s favorite libraries to get started in deep learning.

### Image Databunches
Image Data-bunches help to bring together our training, validation, and test data and process the data by performing all the required transformations and normalizing the image data.

### LR Find
Learning rates can influence our model on how quickly the model learns and adapts itself to the problem. A low learning rate slows the convergence of the training process and a high learning rate can cause unpleasant divergence in the performance. Therefore, good learning rates are vital for the satisfactory performance of a model, and finding optimal learning rates is like looking for a needle in the haystack. Fast.ai’s “lr_find()” is our Knight in shining armor which saves us from the distress of finding good learning rates.

## Trainning the Base Line Model

### Installing Dependencies and setting up the environment for trainning
"""

!pip install -Uqq fastai
!pip install timm

from fastdownload import download_url
from fastai.vision.all import*
from fastcore.all import *

path = '/content/drive/MyDrive/waste_management'

"""### Baseline Model  Resnet 18

Description
ResNet-18 is a convolutional neural network that is 18 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 224-by-224.
"""

## Creating a DataBlock  from the provided Dataset 

##  what is a datablock ?

### from the datablock we are creating a dataloader

dls = DataBlock(
    blocks=(ImageBlock, CategoryBlock), 
    get_items=get_image_files, 
    ## splitter splits the data into trainning and validation percentages
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    # this function assigns the images the labels of the directory they are in
    get_y=parent_label,
    item_tfms=[Resize(192, method='squish')]
).dataloaders(path, bs=32)

## Loking into the dataloader to see a few examples
dls.show_batch(max_n=12)

## Creating Machine vision Learner 
learn = vision_learner(dls, resnet18, metrics=[error_rate,accuracy],pretrained=False)
learn.fine_tune(6)

"""### Testing the Baseline Model on sample Images"""

is_,_,probs = learn.predict(PILImage.create(f'{path}/glass/glass79.jpg'))
print(f"This is a: {is_}.")
print(f"Probability it's glass: {probs[1]:.4f}")

from PIL import Image as PImage
from matplotlib import pyplot as plt
img = PImage.open(f'{path}/glass/glass79.jpg')
plt.imshow(img)
plt.axis('off')

is_,_,probs = learn.predict(PILImage.create(f'{path}/metal/metal7.jpg'))
print(f"This is a: {is_}.")
print(f"Probability it's glass: {probs[2]:.4f}")

from PIL import Image as PImage
from matplotlib import pyplot as plt
img = PImage.open(f'{path}/metal/metal7.jpg')
plt.imshow(img)
plt.axis('off')

"""### Checking the the learner results to see where it was most confused"""

interp = ClassificationInterpretation.from_learner(learn)
interp.plot_top_losses(12)

### Saving the model for later use
learn.export("waste_detector.pkl")

"""This dataset is simple that our Resnet 18 baseline model is getting an accuracy of 92 %

# Improving Results

In the above sections we saw the baseline model achieving an accuracy of 92% lets see if we can improve that further

ResNet-50 is a convolutional neural network that is 50 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 224-by-224

### Resnet 50
"""

## Creating Machine vision Learner 
res_learn = vision_learner(dls, resnet50, metrics=[error_rate,accuracy])
res_learn.fine_tune(5)

"""### Checking the the learner results to see where it was most confused"""

interp = ClassificationInterpretation.from_learner(res_learn)
interp.plot_top_losses(12)

### Saving the model for later use
res_learn.export("waste_detector_50.pkl")

"""Trainning with resnet 50 increased the accuracy from 92%  to 96 % . This data is a simple dataset and our accuracies are high already . with a complex dataset  we would have seen resnet 50  outperform rensnet 18  by alot

### Densnet201
"""

dens_learner = vision_learner(dls, densenet201, metrics=[error_rate,accuracy],pretrained=False)
dens_learner.fine_tune(4)

"""#### learning rate of 1"""

dens_learner = vision_learner(dls, densenet201, metrics=[error_rate,accuracy],pretrained=False)
dens_learner.fine_tune(4,1)

"""With a learning rate of 1  we are only 47% accurate

#### learning rate 0.5
"""

dens_learner = vision_learner(dls, densenet201, metrics=[error_rate,accuracy],pretrained=False)
dens_learner.fine_tune(4 ,0.5)

"""#### Optimal learning rate"""

dens_learner.lr_find(suggest_funcs=(valley, slide))

dens_learner = vision_learner(dls, densenet201, metrics=[error_rate,accuracy])
dens_learner.fine_tune(4 ,1e-2)

"""From the above with detectron we see that tunning the learning rate massively increases the accuracy from around 60%  to 90% for this dataset

### Convnext
"""

arch = 'convnext_small_in22k'

def train(arch, item, batch, epochs=5,lr=1):
    dls = DataBlock(
    blocks=(ImageBlock, CategoryBlock), 
    get_items=get_image_files, 
    ## splitter splits the data into trainning and validation percentages
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    # this function assigns the images the labels of the directory they are in
    get_y=parent_label,
    item_tfms=[Resize(192, method='squish')]
).dataloaders(path, bs=32)
    learn = vision_learner(dls, arch, metrics=error_rate).to_fp16()
    learn.fine_tune(epochs, lr)
    return learn

""" learning rate of 1"""

import timm
convnext_learn = train(arch, item=Resize(192, method='squish'),
              batch=aug_transforms(size=128, min_scale=0.75))

"""The convnext learner doesnt learn anything about the data at  a learning rate of one

learning rate  0.5
"""

convnext_learn = train(arch,lr=0.5 ,item=Resize(192, method='squish'),
              batch=aug_transforms(size=128, min_scale=0.75))

"""At the learning rate of 0.5  the convnext model is around 96% accurate

optimal learning rate
"""

convnext_learn = train(arch,lr=1e-2 ,item=Resize(192, method='squish'),
              batch=aug_transforms(size=128, min_scale=0.75))

"""Given the optimal learning rate the convnext model is 98% accurate for this Dataset . This is the best model and therefore we will Take it to production"""

### Saving the model for later use
convnext_learn.export("waste_detector_c.pkl")

"""# Deployment"""

from IPython.display import Image
Image(f'{path}/dep/final_year.png')